{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1Qr1oOkP2Hm04ec4U3UKf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athishr88/NN_DL/blob/main/Coding_tutorial7/CT7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PART 2: Emotion Classification using word embeddings from a pretrained model (GloVe)\n",
        "\n",
        "## Load the \"emotion\" dataset (input: tweets, output: 6 emotions)\n",
        "import nlp\n",
        "\n",
        "dataset = nlp.load_dataset('emotion')\n",
        "train = dataset['train']\n",
        "val = dataset['validation']\n",
        "test = dataset['test']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUTE-R8oS3Rt",
        "outputId": "c329194b-3393-4934-bdb7-f151ca19cc77"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare input and output pairs for train dataset\n",
        "import numpy as np\n",
        "\n",
        "## Prep the train dataset to samples (input) and labels (output)\n",
        "train_samples = [x['text'] for x in train]\n",
        "train_labels = [x['label'] for x in train]\n",
        "\n",
        "print(\"Classes:\", np.unique(train_labels))\n",
        "print(\"Number of samples in train:\", len(train_samples))\n",
        "print(train_samples[0])\n",
        "\n",
        "## Convert each label in the output to a unique integer\n",
        "classes = list(set(train_labels))\n",
        "class_to_index = dict((c,i) for i, c in enumerate(classes))\n",
        "names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])\n",
        "\n",
        "## Convert the train labels to corresponding int values\n",
        "train_labels = names_to_ids(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K4yWNXzTWMs",
        "outputId": "e3d6736a-a3c2-44cb-ce6e-f9f28ae8da5b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n",
            "Number of samples in train: 16000\n",
            "i didnt feel humiliated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prep the val dataset\n",
        "val_samples = [x['text'] for x in val]\n",
        "val_labels = [x['label'] for x in val]\n",
        "val_labels = names_to_ids(val_labels)\n",
        "\n",
        "## Prep the test dataset\n",
        "test_samples = [x['text'] for x in test]\n",
        "test_labels = [x['label'] for x in test]\n",
        "test_labels = names_to_ids(test_labels)"
      ],
      "metadata": {
        "id": "4apZBlHCUJyg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create our Text Vectorizer to index our vocabulary based on the train samples \n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=100)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128) ## Read batches of 128 samples\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "dBrUEiW7VQ-1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Print out top five words in the vocab\n",
        "print(len(vectorizer.get_vocabulary())) ## We set max_tokens=10000\n",
        "vectorizer.get_vocabulary()[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODMjIMmR155w",
        "outputId": "45fb0d1a-436a-4233-a0a7-76756eca216c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'i', 'feel', 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Text an example of what a string looks like after vectorization\n",
        "output = vectorizer([[\"I feel good today\"]])\n",
        "output.numpy()[0, :4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPUZhbQQ2A54",
        "outputId": "bbb413f1-b09c-4a36-dfa9-c0592a137eb4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,   3, 101, 122])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a map to get the unique list of the vocabulary\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "\n",
        "## print the unique list of integers for the same string using the new map \"Word_index\"\n",
        "test = [\"i\", \"feel\", \"good\", \"today\"]\n",
        "[word_index[w] for w in test]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AisOXZoR2Sro",
        "outputId": "7b621fff-917a-4c7c-cc18-1ba72deffc41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 101, 122]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Vectorize our data (Convert the string data to integer data)\n",
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ],
      "metadata": {
        "id": "6yYPeUyy4n01"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Download and unzip the Stanford GloVe model (pretrained word embeddings)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "rjVY3xtRT7cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read the embeddings in the pretrained model (we are using the 100D version of GloVe)\n",
        "import os\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEPOW2RQ55tF",
        "outputId": "3c79893e-1734-4146-d268-cd3368737286"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create \"embedding_matrix\" to index our vocabulary using the GloVe model \n",
        "num_tokens = len(voc) \n",
        "embedding_dim = 100 ## 100 dimensions\n",
        "hits = 0 ## number of words that were found in the pretrained model\n",
        "misses = 0 ## number of words that were missing in the pretrained model\n",
        "\n",
        "# Prepare embedding matrix for our word list\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLpPAPJ1A7lD",
        "outputId": "be4552ef-f505-4520-b188-a464ee85961f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 9627 words (373 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define our embedding layer for the training model \n",
        "## We load our embedding_matrix as the initializer and set trainable to False to avoid retraining this layer\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.initializers import Constant\n",
        "\n",
        "embedding_layer = Embedding(num_tokens, embedding_dim,\n",
        "                            embeddings_initializer= Constant(embedding_matrix), \n",
        "                            trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "FH_IRuQbCr7y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a simple Bidirectional LSTM model\n",
        "\n",
        "from keras import layers, Input, Model\n",
        "\n",
        "int_sequences_input = Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Bidirectional(layers.LSTM(20, return_sequences=True))(embedded_sequences)\n",
        "x = layers.Bidirectional(layers.LSTM(20))(x)\n",
        "preds = layers.Dense(len(classes), activation=\"softmax\")(x)\n",
        "model = Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi3mdJXcK_ae",
        "outputId": "7968a82c-4257-4204-b96f-57eab574f054"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 100)         1000000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, None, 40)         19360     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 40)               9760      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 246       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,029,366\n",
            "Trainable params: 29,366\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the model \n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k-XEhk6Om91",
        "outputId": "a0a89f75-268a-4789-ebc6-bd24581e78a2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "125/125 [==============================] - 30s 176ms/step - loss: 1.5358 - acc: 0.3925 - val_loss: 1.3928 - val_acc: 0.5015\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 20s 156ms/step - loss: 1.2671 - acc: 0.5321 - val_loss: 1.1504 - val_acc: 0.5780\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 24s 195ms/step - loss: 1.1092 - acc: 0.5865 - val_loss: 1.0286 - val_acc: 0.6070\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 19s 153ms/step - loss: 0.9928 - acc: 0.6319 - val_loss: 0.9235 - val_acc: 0.6680\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 18s 147ms/step - loss: 0.8714 - acc: 0.6856 - val_loss: 0.8025 - val_acc: 0.7275\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 0.7444 - acc: 0.7421 - val_loss: 0.6881 - val_acc: 0.7635\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 18s 145ms/step - loss: 0.6288 - acc: 0.7861 - val_loss: 0.5684 - val_acc: 0.8095\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 20s 164ms/step - loss: 0.5175 - acc: 0.8248 - val_loss: 0.5137 - val_acc: 0.8235\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 0.4515 - acc: 0.8474 - val_loss: 0.4650 - val_acc: 0.8415\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 19s 151ms/step - loss: 0.3876 - acc: 0.8664 - val_loss: 0.4186 - val_acc: 0.8600\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 19s 154ms/step - loss: 0.3543 - acc: 0.8763 - val_loss: 0.3841 - val_acc: 0.8730\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.3115 - acc: 0.8922 - val_loss: 0.3769 - val_acc: 0.8750\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 19s 148ms/step - loss: 0.2904 - acc: 0.8993 - val_loss: 0.3518 - val_acc: 0.8790\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 18s 147ms/step - loss: 0.2706 - acc: 0.9052 - val_loss: 0.3689 - val_acc: 0.8790\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 20s 157ms/step - loss: 0.2483 - acc: 0.9109 - val_loss: 0.3402 - val_acc: 0.8815\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 19s 156ms/step - loss: 0.2275 - acc: 0.9171 - val_loss: 0.3161 - val_acc: 0.8920\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 19s 152ms/step - loss: 0.2068 - acc: 0.9252 - val_loss: 0.3066 - val_acc: 0.8920\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 19s 149ms/step - loss: 0.1994 - acc: 0.9259 - val_loss: 0.2952 - val_acc: 0.8960\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 19s 150ms/step - loss: 0.1874 - acc: 0.9302 - val_loss: 0.3854 - val_acc: 0.8705\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 19s 153ms/step - loss: 0.1865 - acc: 0.9314 - val_loss: 0.2987 - val_acc: 0.8945\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f77b8643ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test the model on a sample tweet from the test split\n",
        "\n",
        "## Export a Model object to read a string of arbitary length\n",
        "string_input = Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input) \n",
        "preds = model(x)\n",
        "end_to_end_model = Model(string_input, preds)\n",
        "\n",
        "## Predict using the model\n",
        "probabilities = end_to_end_model.predict([test_samples[11]])\n",
        "\n",
        "print(\"String: {}\".format(test_samples[11]))\n",
        "print(\"Target output: {}\".format(classes[test_labels[11]]))\n",
        "print(\"Predicted output: {}\".format(classes[np.argmax(probabilities[0])]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2bOK5hXUJ3d",
        "outputId": "ef848a95-943c-427e-fe55-f32bbc524a2b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f77b4467dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "String: i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey\n",
            "Target output: sadness\n",
            "Predicted output: sadness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PART 3: Semantic Analogies using Gensim library\n",
        "## Gensim is an open source Python library for NLP\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98egPPEkXCso",
        "outputId": "c02f9282-87a3-474a-9760-267b64832d44"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Use builtin function in Gensim to convert glove to word2vec format \n",
        "## (Gensim works on Word2Vec and has built in function to convert GloVe to Word2Vec)\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdPSFttRXdLA",
        "outputId": "dd7593ed-78cd-4178-beb6-bb1390878bfb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test with semantic analogies\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "# load the GloVe model \n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "# Print example: (king - man) + woman = ? \n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iw2KAmsXqZu",
        "outputId": "cd96554c-f72b-4f96-f1a2-503a5e315970"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7698540687561035)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Print another example\n",
        "model.most_similar(positive=[\"moscow\", \"france\"], negative = \"russia\", topn = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tynGrrZWYK3n",
        "outputId": "90219f71-eaa6-4ecb-b49a-70dc3d8c81ba"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('paris', 0.8822440505027771)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "553HOBHyYZRV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}