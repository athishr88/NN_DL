{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT6.ipynb",
      "provenance": [],
      "mount_file_id": "1DP0XSiwMxnIVZCIsQJohJo-JmseKc1FS",
      "authorship_tag": "ABX9TyMiT3kNQ4XCSGNMNoBFKw28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athishr88/NN_DL/blob/main/Coding_tutorial6/CT6_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O0bvrA5OrcO",
        "outputId": "f50409bf-2b87-41ac-e715-3bb342328ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 1us/step\n",
            "614400/600901 [==============================] - 0s 1us/step\n"
          ]
        }
      ],
      "source": [
        "# LSTM\n",
        "\n",
        "#1. Load sample text file\n",
        "import tensorflow as tf\n",
        "path_to_file = tf.keras.utils.get_file(\"nietzsche.txt\", \n",
        "                  origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Read the text\n",
        "text = open(path_to_file, \"rb\").read().decode(encoding='utf-8')\n",
        "text = text.lower()\n",
        "\n",
        "# Number of characters in the text\n",
        "n_char = len(text)\n",
        "print(f\"Length of text: {n_char} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecgAGnvkOxat",
        "outputId": "f94a8354-0fce-4539-c2ce-ea5947a068d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 600893 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. List of unique characters\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "num_vocab = len(vocab)\n",
        "num_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws5Bfq8aO0Qi",
        "outputId": "29fa226f-c9cf-43ca-fd39-35051ad1713d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Create a mapping for each text\n",
        "char_to_int = dict((c,i) for i, c in enumerate(vocab))\n",
        "\n",
        "int_to_char = dict((i,c) for i, c in enumerate(vocab))"
      ],
      "metadata": {
        "id": "p8loCZGVO1lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Prepare the dataset of input to output pairs\n",
        "## \"Hello\" --> input: \"Hell\" output: \"o\"\n",
        "\n",
        "seq_length = 40\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_char-seq_length, 1):\n",
        "    seq_in = text[i:i+seq_length] # Inout substring\n",
        "    seq_out = text[i+seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "    \n",
        "n_patterns = len(dataX)\n",
        "print(f\"Total patterns :{n_patterns}\")\n",
        "len(dataX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTfqwFg5O2xu",
        "outputId": "74889147-ff39-4a68-edb2-bcf000379014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patterns :600853\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600853"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3Ue1FeHO44C",
        "outputId": "7116a338-b6d9-47ea-e4ee-f0167619135d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600853"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Preprocessing the input data\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Reshape X to be [Samples, time_steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalize\n",
        "X = X/float(num_vocab)\n",
        "\n",
        "# One hot encode\n",
        "y = to_categorical(dataY)\n",
        "\n"
      ],
      "metadata": {
        "id": "SyAYiiLDO6av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ-8RwH6O7eK",
        "outputId": "ccda60f2-6528-499d-f218-37fcd8466ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600853, 40, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Build the model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "2aksChVPO8zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Train and save model\n",
        "\n",
        "filepath = \"LSTM_tutorial_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "history = model.fit(X, y, epochs=20, batch_size=128, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "VE6bPgvPO98b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Loading weights\n",
        "model.load_weights(filepath)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "qwHgfJc7PAY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Predict characters using model\n",
        "import sys\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Inpput:\")\n",
        "print(\"\\\"\", \"\".join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"Predicted: \")\n",
        "print(\"----------\")\n",
        "\n",
        "# Generate characters\n",
        "for i in range(100):\n",
        "  x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x/float(num_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = np.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\n-----------------\")"
      ],
      "metadata": {
        "id": "jwt3CkXCZIFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "8bUc4uWLaK06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}